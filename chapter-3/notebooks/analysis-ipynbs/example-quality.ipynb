{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spider Evaluation Matrix Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spider All Experiments Evaluation Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8272/8272 [02:46<00:00, 49.78it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2268, 4046, 277, 1681]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "import glob\n",
    "from tqdm import tqdm\n",
    "from utils.utils import sql_similarity\n",
    "from utils.utils import mask_query\n",
    "\n",
    "pattern = os.path.join('chapter-3', 'results', 'spider', '**', '*results-1.json')\n",
    "\n",
    "# Retrieve all matching file paths\n",
    "results_files = glob.glob(pattern, recursive=True)\n",
    "results_files.remove(os.path.join('chapter-3', 'results', 'spider', 'random-experiments', '1-shot', 'random-results-1.json'))\n",
    "\n",
    "\n",
    "def load_json(file_path):\n",
    "    with open(file_path, 'r') as f:\n",
    "        return json.load(f)\n",
    "\n",
    "# Load all result data\n",
    "matrix = [0,0,0,0]\n",
    "results_data = {os.path.basename(file_path): load_json(file_path).get('questions', []) for file_path in results_files}\n",
    "\n",
    "# Collect all questions into a single list\n",
    "all_questions = []\n",
    "for questions in results_data.values():\n",
    "    all_questions.extend(questions)\n",
    "\n",
    "for question in tqdm(all_questions):\n",
    "    correct = question.get('correct')\n",
    "    ex_sql = question['examples'][0]['query']\n",
    "    gold_sql = 'SELECT ' + question['response']\n",
    "    \n",
    "    similarity = sql_similarity(mask_query(ex_sql), mask_query(gold_sql))\n",
    "    #print(ex_sql,\"/\", gold_sql, similarity, \"\\n\")\n",
    "    \n",
    "    if correct == 1:\n",
    "        if similarity >= 0.75:\n",
    "            matrix[0] += 1\n",
    "        else:\n",
    "            matrix[1] += 1\n",
    "    elif correct == 0:\n",
    "        if similarity > 0.75:\n",
    "            matrix[2] += 1\n",
    "        else:\n",
    "            matrix[3] += 1\n",
    "\n",
    "print(matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spider Average SQLSim(ex, gold) Scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of questions where correct = 0: 1958\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1958/1958 [00:39<00:00, 49.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean average sql_similarity score for questions where correct = 0: 0.4813593776326583\n",
      "Number of questions where correct = 1: 6314\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6314/6314 [01:07<00:00, 93.36it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean average sql_similarity score for questions where correct = 1: 0.6144154839161518\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "import glob\n",
    "from tqdm import tqdm\n",
    "from utils.utils import sql_similarity, mask_query\n",
    "import numpy as np\n",
    "\n",
    "pattern = os.path.join('chapter-3', 'results', 'spider', '**', '*results-1.json')\n",
    "\n",
    "# Retrieve all matching file paths\n",
    "results_files = glob.glob(pattern, recursive=True)\n",
    "results_files.remove(os.path.join('chapter-3', 'results', 'spider', 'random-experiments', '1-shot', 'random-results-1.json'))\n",
    "\n",
    "def load_json(file_path):\n",
    "    with open(file_path, 'r') as f:\n",
    "        return json.load(f)\n",
    "\n",
    "# Load all result data\n",
    "results_data = {os.path.basename(file_path): load_json(file_path).get('questions', []) for file_path in results_files}\n",
    "\n",
    "# Collect all questions into a single list\n",
    "all_questions = []\n",
    "for questions in results_data.values():\n",
    "    all_questions.extend(questions)\n",
    "\n",
    "# Filter out questions where correct = 0\n",
    "incorrect_questions = [question for question in all_questions if question.get('correct') == 0]\n",
    "\n",
    "# Verify the number of incorrect questions\n",
    "print(f\"Number of questions where correct = 0: {len(incorrect_questions)}\")\n",
    "\n",
    "# Initialize the list to store sql_similarity scores for questions where correct = 0\n",
    "similarity_scores = []\n",
    "\n",
    "# Loop through all incorrect questions and measure sql_similarity\n",
    "for question in tqdm(incorrect_questions):\n",
    "    ex_sql = question['examples'][0]['query']\n",
    "    gold_sql = 'SELECT ' + question['response']\n",
    "    \n",
    "    similarity = sql_similarity(mask_query(ex_sql), mask_query(gold_sql))\n",
    "    similarity_scores.append(similarity)\n",
    "\n",
    "# Compute the mean average sql_similarity score\n",
    "mean_similarity = np.mean(similarity_scores)\n",
    "\n",
    "print(f\"Mean average sql_similarity score for questions where correct = 0: {mean_similarity}\")\n",
    "\n",
    "# ---------------------------------------------------------------------------------------------------------\n",
    "\n",
    "# Filter out questions where correct = 1\n",
    "correct_questions = [question for question in all_questions if question.get('correct') == 1]\n",
    "\n",
    "# Verify the number of incorrect questions\n",
    "print(f\"Number of questions where correct = 1: {len(correct_questions)}\")\n",
    "\n",
    "# Initialize the list to store sql_similarity scores for questions where correct = 0\n",
    "similarity_scores = []\n",
    "\n",
    "# Loop through all incorrect questions and measure sql_similarity\n",
    "for question in tqdm(correct_questions):\n",
    "    ex_sql = question['examples'][0]['query']\n",
    "    gold_sql = 'SELECT ' + question['response']\n",
    "    \n",
    "    similarity = sql_similarity(mask_query(ex_sql), mask_query(gold_sql))\n",
    "    similarity_scores.append(similarity)\n",
    "\n",
    "# Compute the mean average sql_similarity score\n",
    "mean_similarity = np.mean(similarity_scores)\n",
    "\n",
    "print(f\"Mean average sql_similarity score for questions where correct = 1: {mean_similarity}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chapter-3\\results\\spider\\all-MiniLM-L12-v2-experiments\\1-shot\\all-MiniLM-L12-v2-results-1.json 0.5403660407648827\n",
      "chapter-3\\results\\spider\\all-MiniLM-L12-v2-experiments\\3-shot\\all-MiniLM-L12-v2-results-3.json 0.5275907423338566\n",
      "chapter-3\\results\\spider\\all-MiniLM-L12-v2-experiments\\5-shot\\all-MiniLM-L12-v2-results-5.json 0.5216673467770376\n",
      "chapter-3\\results\\spider\\all-MiniLM-L6-v2-experiments\\1-shot\\all-MiniLM-L6-v2-results-1.json 0.5269915784700737\n",
      "chapter-3\\results\\spider\\all-MiniLM-L6-v2-experiments\\3-shot\\all-MiniLM-L6-v2-results-3.json 0.5108134680056198\n",
      "chapter-3\\results\\spider\\all-MiniLM-L6-v2-experiments\\5-shot\\all-MiniLM-L6-v2-results-5.json 0.5069976256615013\n",
      "chapter-3\\results\\spider\\all-mpnet-base-v2-experiments\\1-shot\\all-mpnet-base-v2-results-1.json 0.5450070802597946\n",
      "chapter-3\\results\\spider\\all-mpnet-base-v2-experiments\\3-shot\\all-mpnet-base-v2-results-3.json 0.5251970974439\n",
      "chapter-3\\results\\spider\\all-mpnet-base-v2-experiments\\5-shot\\all-mpnet-base-v2-results-5.json 0.5171235543404186\n",
      "chapter-3\\results\\spider\\bert-base-nli-mean-tokens-experiments\\1-shot\\bert-base-nli-mean-tokens-results-1.json 0.5342671459734862\n",
      "chapter-3\\results\\spider\\bert-base-nli-mean-tokens-experiments\\3-shot\\bert-base-nli-mean-tokens-results-3.json 0.5288436968072391\n",
      "chapter-3\\results\\spider\\bert-base-nli-mean-tokens-experiments\\5-shot\\bert-base-nli-mean-tokens-results-5.json 0.5187756157302681\n",
      "chapter-3\\results\\spider\\random-experiments\\1-shot\\random-results-1.json 0.33550009781488466\n",
      "chapter-3\\results\\spider\\random-experiments\\3-shot\\random-results-3.json 0.3340384757973717\n",
      "chapter-3\\results\\spider\\random-experiments\\5-shot\\random-results-5.json 0.34019255744112503\n",
      "chapter-3\\results\\spider\\stsb-roberta-base-experiments\\1-shot\\stsb-roberta-base-results-1.json 0.5433172994569394\n",
      "chapter-3\\results\\spider\\stsb-roberta-base-experiments\\3-shot\\stsb-roberta-base-results-3.json 0.5265505372873481\n",
      "chapter-3\\results\\spider\\stsb-roberta-base-experiments\\5-shot\\roBERTa-results-5.json 0.5191956349100878\n",
      "chapter-3\\results\\spider\\stsb-roberta-base-experiments\\5-shot\\stsb-roberta-base-results-5.json 0.5191956349100878\n",
      "chapter-3\\results\\spider\\text-embedding-3-large-experiments\\1-shot\\text-embedding-3-large-results-1.json 0.4615851942493405\n",
      "chapter-3\\results\\spider\\text-embedding-3-large-experiments\\3-shot\\text-embedding-3-large-results-3.json 0.45544162568623964\n",
      "chapter-3\\results\\spider\\text-embedding-3-large-experiments\\5-shot\\text-embedding-3-large-results-5.json 0.44874195120219484\n",
      "chapter-3\\results\\spider\\text-embedding-3-small-experiments\\1-shot\\text-embedding-3-small-results-1.json 0.44836180061726477\n",
      "chapter-3\\results\\spider\\text-embedding-3-small-experiments\\3-shot\\text-embedding-3-small-results-3.json 0.4455297102666525\n",
      "chapter-3\\results\\spider\\text-embedding-3-small-experiments\\5-shot\\text-embedding-3-small-results-5.json 0.4391691293897495\n",
      "chapter-3\\results\\spider\\text-embedding-ada-002-experiments\\1-shot\\text-embedding-ada-002-results-1.json 0.4704074515249843\n",
      "chapter-3\\results\\spider\\text-embedding-ada-002-experiments\\3-shot\\text-embedding-ada-002-results-3.json 0.46943413373682275\n",
      "chapter-3\\results\\spider\\text-embedding-ada-002-experiments\\5-shot\\text-embedding-ada-002-results-5.json 0.46499971074609475\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import glob\n",
    "import os\n",
    "from utils.utils import mask_query, sql_similarity\n",
    "\n",
    "def calculate_mean_example_quality(file_path):\n",
    "    # Load the JSON file\n",
    "    with open(file_path, 'r') as file:\n",
    "        data = json.load(file)\n",
    "    \n",
    "    total_quality = 0\n",
    "    total_questions = len(data['questions'])\n",
    "    \n",
    "    for question in data['questions']:\n",
    "        example_qualities = []\n",
    "        for example in question['examples']:\n",
    "            masked_query = 'SELECT' + mask_query(question['response'])\n",
    "            masked_example_query = mask_query(example['query'])\n",
    "            quality = sql_similarity(masked_query, masked_example_query)\n",
    "            example_qualities.append(quality)\n",
    "        \n",
    "        mean_quality = sum(example_qualities) / len(example_qualities)\n",
    "        total_quality += mean_quality\n",
    "    \n",
    "    overall_mean_quality = total_quality / total_questions\n",
    "    return overall_mean_quality\n",
    "\n",
    "\n",
    "# Retrieve all local json results-file paths from completed Spider runs.\n",
    "pattern = os.path.join('chapter-3','results','spider', '**', '*results-[12345].json')\n",
    "#\n",
    "results = glob.glob(pattern, recursive=True)\n",
    "\n",
    "# Tally up how many of the 1034 Spider questions have been answered correctly\n",
    "\n",
    "for file_path in results:\n",
    "    print(file_path, calculate_mean_example_quality(file_path))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spider % of Questions with a 'Good' Example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0.75 Threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1034/1034 [4:59:14<00:00, 17.36s/it]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Proportion of Spider test questions with a 'good' training example: 0.92\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from utils.utils import sql_similarity, mask_query\n",
    "from tqdm import tqdm\n",
    "from utils.data.data_builder import load_data\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.metrics.pairwise import euclidean_distances\n",
    "import json\n",
    "import numpy as np\n",
    "\n",
    "path_data = \"benchmarks\"\n",
    "\n",
    "data = load_data(\"spider\", path_data)\n",
    "\n",
    "embedding = SentenceTransformer('s2593817/sft-sql-embedding')\n",
    "\n",
    "def read_sql_queries(file_path, has_db_name=False):\n",
    "    with open(file_path, 'r') as file:\n",
    "        lines = file.readlines()\n",
    "    if has_db_name:\n",
    "        queries = [line.split('\\t')[0].strip() for line in lines]\n",
    "    else:\n",
    "        queries = [line.strip() for line in lines]\n",
    "    return queries\n",
    "\n",
    "def read_sql_queries_json(file_path):\n",
    "    with open(file_path, 'r') as file:\n",
    "        data = json.load(file)\n",
    "    return  [item['query'] for item in data]\n",
    "\n",
    "def calculate_proportion(dev_file, train_gold_file, threshold=0.85):\n",
    "    dev_queries = read_sql_queries(dev_file)\n",
    "    train_gold_queries = read_sql_queries_json(train_gold_file)\n",
    "    \n",
    "    masked_train = [mask_query(query) for query in train_gold_queries]\n",
    "    embeddings_train = embedding.encode(masked_train)\n",
    "    \n",
    "    matching_count = 0\n",
    "    \n",
    "    for dev_query in tqdm(dev_queries):\n",
    "        \n",
    "        # Firstly order candidates by fine-tuned SQL embedding distances\n",
    "        masked_dev = mask_query(dev_query)\n",
    "        embedding_dev = embedding.encode(masked_dev).reshape(1, -1)\n",
    "        \n",
    "        distances = np.squeeze(euclidean_distances(embedding_dev, embeddings_train)).tolist()\n",
    "        pairs = [(distance, index) for distance, index in zip(distances, range(len(distances)))]\n",
    "\n",
    "        pairs_sorted = sorted(pairs, key=lambda x: x[0])\n",
    "        for d, index in pairs_sorted:\n",
    "            # Determine the SQLSim value and if it is above the threshold, record a success and move on to the next question\n",
    "            if sql_similarity(masked_dev, masked_train[index]) > 0.75:\n",
    "                matching_count += 1\n",
    "                break\n",
    "    \n",
    "    proportion = matching_count / len(dev_queries)\n",
    "    return proportion\n",
    "\n",
    "dev_file_path = 'benchmarks/spider/dev_gold.sql'\n",
    "train_gold_file_path = 'benchmarks/spider/train_spider_and_others.json'\n",
    "\n",
    "proportion = calculate_proportion(dev_file_path, train_gold_file_path)\n",
    "print(f\"Proportion of Spider test questions with a 'good' training example: {proportion:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0.85 Threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\MDrap\\anaconda3\\Lib\\site-packages\\huggingface_hub\\file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "100%|██████████| 1034/1034 [06:47<00:00,  2.54it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Proportion of Spider test questions with a 'good' training example: 0.66\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from utils.utils import sql_similarity, mask_query\n",
    "from tqdm import tqdm\n",
    "from utils.data.data_builder import load_data\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.metrics.pairwise import euclidean_distances\n",
    "import json\n",
    "import numpy as np\n",
    "\n",
    "path_data = \"benchmarks\"\n",
    "\n",
    "data = load_data(\"spider\", path_data)\n",
    "\n",
    "embedding = SentenceTransformer('s2593817/sft-sql-embedding')\n",
    "\n",
    "def read_sql_queries(file_path, has_db_name=False):\n",
    "    with open(file_path, 'r') as file:\n",
    "        lines = file.readlines()\n",
    "    if has_db_name:\n",
    "        queries = [line.split('\\t')[0].strip() for line in lines]\n",
    "    else:\n",
    "        queries = [line.strip() for line in lines]\n",
    "    return queries\n",
    "\n",
    "def read_sql_queries_json(file_path):\n",
    "    with open(file_path, 'r') as file:\n",
    "        data = json.load(file)\n",
    "    return  [item['query'] for item in data]\n",
    "\n",
    "def calculate_proportion(dev_file, train_gold_file, threshold=0.85):\n",
    "    dev_queries = read_sql_queries(dev_file)\n",
    "    train_gold_queries = read_sql_queries_json(train_gold_file)\n",
    "    \n",
    "    masked_train = [mask_query(query) for query in train_gold_queries]\n",
    "    embeddings_train = embedding.encode(masked_train)\n",
    "    \n",
    "    matching_count = 0\n",
    "    \n",
    "    for dev_query in tqdm(dev_queries):\n",
    "        \n",
    "        # Firstly order candidates by fine-tuned SQL embedding distances\n",
    "        masked_dev = mask_query(dev_query)\n",
    "        embedding_dev = embedding.encode(masked_dev).reshape(1, -1)\n",
    "        \n",
    "        distances = np.squeeze(euclidean_distances(embedding_dev, embeddings_train)).tolist()\n",
    "        pairs = [(distance, index) for distance, index in zip(distances, range(len(distances)))]\n",
    "\n",
    "        pairs_sorted = sorted(pairs, key=lambda x: x[0])\n",
    "        for d, index in pairs_sorted:\n",
    "            # Determine the SQLSim value and if it is above the threshold, record a success and move on to the next question\n",
    "            if sql_similarity(masked_dev, masked_train[index]) > 0.85:\n",
    "                matching_count += 1\n",
    "                break\n",
    "    \n",
    "    proportion = matching_count / len(dev_queries)\n",
    "    return proportion\n",
    "\n",
    "dev_file_path = 'benchmarks/spider/dev_gold.sql'\n",
    "train_gold_file_path = 'benchmarks/spider/train_spider_and_others.json'\n",
    "\n",
    "proportion = calculate_proportion(dev_file_path, train_gold_file_path)\n",
    "print(f\"Proportion of Spider test questions with a 'good' training example: {proportion:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BIRD Evaluation Matrix Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BIRD All Experiments Evaluation Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 12272/12272 [11:56<00:00, 17.14it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[767, 4533, 544, 6428]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "import glob\n",
    "from tqdm import tqdm\n",
    "from utils.utils import sql_similarity\n",
    "from utils.utils import mask_query\n",
    "\n",
    "pattern = os.path.join('chapter-3', 'results', 'bird', '**', '*results-1.json')\n",
    "\n",
    "# Retrieve all matching file paths\n",
    "results_files = glob.glob(pattern, recursive=True)\n",
    "results_files.remove(os.path.join('chapter-3', 'results', 'bird', 'random-experiments', '1-shot', 'random-results-1.json'))\n",
    "\n",
    "def load_json(file_path):\n",
    "    with open(file_path, 'r') as f:\n",
    "        return json.load(f)\n",
    "\n",
    "# Load all result data\n",
    "matrix = [0,0,0,0]\n",
    "results_data = {os.path.basename(file_path): load_json(file_path).get('questions', []) for file_path in results_files}\n",
    "\n",
    "# Collect all questions into a single list\n",
    "all_questions = []\n",
    "for questions in results_data.values():\n",
    "    all_questions.extend(questions)\n",
    "\n",
    "for question in tqdm(all_questions):\n",
    "    correct = question.get('correct')\n",
    "    ex_sql = question['examples'][0]['query']\n",
    "    gold_sql = 'SELECT ' + question['response']\n",
    "    \n",
    "    similarity = sql_similarity(mask_query(ex_sql), mask_query(gold_sql))\n",
    "    #print(ex_sql,\"/\", gold_sql, similarity, \"\\n\")\n",
    "    \n",
    "    if correct == 1:\n",
    "        if similarity >= 0.75:\n",
    "            matrix[0] += 1\n",
    "        else:\n",
    "            matrix[1] += 1\n",
    "    elif correct == 0:\n",
    "        if similarity > 0.75:\n",
    "            matrix[2] += 1\n",
    "        else:\n",
    "            matrix[3] += 1\n",
    "\n",
    "print(matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BIRD Average SQLSim(ex, gold) Scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of questions where correct = 0: 6972\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6972/6972 [13:53<00:00,  8.36it/s]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean average sql_similarity score for questions where correct = 0: 0.47058515484842145\n",
      "Number of questions where correct = 1: 5300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5300/5300 [07:49<00:00, 11.30it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean average sql_similarity score for questions where correct = 1: 0.505176271572087\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "import glob\n",
    "from tqdm import tqdm\n",
    "from utils.utils import sql_similarity, mask_query\n",
    "import numpy as np\n",
    "\n",
    "pattern = os.path.join('chapter-3', 'results', 'bird', '**', '*results-1.json')\n",
    "\n",
    "# Retrieve all matching file paths\n",
    "results_files = glob.glob(pattern, recursive=True)\n",
    "results_files.remove(os.path.join('chapter-3', 'results', 'bird', 'random-experiments', '1-shot', 'random-results-1.json'))\n",
    "\n",
    "def load_json(file_path):\n",
    "    with open(file_path, 'r') as f:\n",
    "        return json.load(f)\n",
    "\n",
    "# Load all result data\n",
    "results_data = {os.path.basename(file_path): load_json(file_path).get('questions', []) for file_path in results_files}\n",
    "\n",
    "# Collect all questions into a single list\n",
    "all_questions = []\n",
    "for questions in results_data.values():\n",
    "    all_questions.extend(questions)\n",
    "\n",
    "# Filter out questions where correct = 0\n",
    "incorrect_questions = [question for question in all_questions if question.get('correct') == 0]\n",
    "\n",
    "# Verify the number of incorrect questions\n",
    "print(f\"Number of questions where correct = 0: {len(incorrect_questions)}\")\n",
    "\n",
    "# Initialize the list to store sql_similarity scores for questions where correct = 0\n",
    "similarity_scores = []\n",
    "\n",
    "# Loop through all incorrect questions and measure sql_similarity\n",
    "for question in tqdm(incorrect_questions):\n",
    "    ex_sql = question['examples'][0]['query']\n",
    "    gold_sql = 'SELECT ' + question['response']\n",
    "    \n",
    "    similarity = sql_similarity(mask_query(ex_sql), mask_query(gold_sql))\n",
    "    similarity_scores.append(similarity)\n",
    "\n",
    "# Compute the mean average sql_similarity score\n",
    "mean_similarity = np.mean(similarity_scores)\n",
    "\n",
    "print(f\"Mean average sql_similarity score for questions where correct = 0: {mean_similarity}\")\n",
    "\n",
    "# ---------------------------------------------------------------------------------------------------------\n",
    "\n",
    "# Filter out questions where correct = 1\n",
    "correct_questions = [question for question in all_questions if question.get('correct') == 1]\n",
    "\n",
    "# Verify the number of incorrect questions\n",
    "print(f\"Number of questions where correct = 1: {len(correct_questions)}\")\n",
    "\n",
    "# Initialize the list to store sql_similarity scores for questions where correct = 0\n",
    "similarity_scores = []\n",
    "\n",
    "# Loop through all incorrect questions and measure sql_similarity\n",
    "for question in tqdm(correct_questions):\n",
    "    ex_sql = question['examples'][0]['query']\n",
    "    gold_sql = 'SELECT ' + question['response']\n",
    "    \n",
    "    similarity = sql_similarity(mask_query(ex_sql), mask_query(gold_sql))\n",
    "    similarity_scores.append(similarity)\n",
    "\n",
    "# Compute the mean average sql_similarity score\n",
    "mean_similarity = np.mean(similarity_scores)\n",
    "\n",
    "print(f\"Mean average sql_similarity score for questions where correct = 1: {mean_similarity}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chapter-3\\results\\bird\\all-MiniLM-L12-v2-experiments\\1-shot\\all-MiniLM-L12-v2-results-1.json 0.4704035114367873\n",
      "chapter-3\\results\\bird\\all-MiniLM-L12-v2-experiments\\3-shot\\all-MiniLM-L12-v2-results-3.json 0.4683763474837952\n",
      "chapter-3\\results\\bird\\all-MiniLM-L12-v2-experiments\\5-shot\\all-MiniLM-L12-v2-results-5.json 0.466927815324469\n",
      "chapter-3\\results\\bird\\all-MiniLM-L6-v2-experiments\\1-shot\\all-MiniLM-L6-v2-results-1.json 0.466358913494554\n",
      "chapter-3\\results\\bird\\all-MiniLM-L6-v2-experiments\\3-shot\\all-MiniLM-L6-v2-results-3.json 0.4666040378936047\n",
      "chapter-3\\results\\bird\\all-MiniLM-L6-v2-experiments\\5-shot\\all-MiniLM-L6-v2-results-5.json 0.46569540947537685\n",
      "chapter-3\\results\\bird\\all-mpnet-base-v2-experiments\\1-shot\\all-mpnet-base-v2-results-1.json 0.4797684322731596\n",
      "chapter-3\\results\\bird\\all-mpnet-base-v2-experiments\\3-shot\\all-mpnet-base-v2-results-3.json 0.48051126065769534\n",
      "chapter-3\\results\\bird\\all-mpnet-base-v2-experiments\\5-shot\\all-mpnet-base-v2-results-5.json 0.480562527221327\n",
      "chapter-3\\results\\bird\\bert-base-nli-mean-tokens-experiments\\1-shot\\bert-base-nli-mean-tokens-results-1.json 0.4678302882307446\n",
      "chapter-3\\results\\bird\\bert-base-nli-mean-tokens-experiments\\3-shot\\bert-base-nli-mean-tokens-results-3.json 0.46820669423168215\n",
      "chapter-3\\results\\bird\\bert-base-nli-mean-tokens-experiments\\5-shot\\bert-base-nli-mean-tokens-results-5.json 0.46941338630047497\n",
      "chapter-3\\results\\bird\\random-experiments\\1-shot\\random-results-1.json 0.4228358448199991\n",
      "chapter-3\\results\\bird\\random-experiments\\3-shot\\random-results-3.json 0.4261271237409239\n",
      "chapter-3\\results\\bird\\random-experiments\\5-shot\\random-results-5.json 0.4241670785289774\n",
      "chapter-3\\results\\bird\\stsb-roberta-base-experiments\\1-shot\\stsb-roberta-base-results-1.json 0.45854621220180525\n",
      "chapter-3\\results\\bird\\stsb-roberta-base-experiments\\3-shot\\stsb-roberta-base-results-3.json 0.4540723824579406\n",
      "chapter-3\\results\\bird\\stsb-roberta-base-experiments\\5-shot\\stsb-roberta-base-results-5.json 0.4528258746753284\n",
      "chapter-3\\results\\bird\\text-embedding-3-large-experiments\\1-shot\\text-embedding-3-large-results-1.json 0.46570963612938965\n",
      "chapter-3\\results\\bird\\text-embedding-3-large-experiments\\3-shot\\text-embedding-3-large-results-3.json 0.46356559816304066\n",
      "chapter-3\\results\\bird\\text-embedding-3-large-experiments\\5-shot\\text-embedding-3-large-results-5.json 0.46219192433469447\n",
      "chapter-3\\results\\bird\\text-embedding-3-small-experiments\\1-shot\\text-embedding-3-small-results-1.json 0.45761920904948156\n",
      "chapter-3\\results\\bird\\text-embedding-3-small-experiments\\3-shot\\text-embedding-3-small-results-3.json 0.4562121335853726\n",
      "chapter-3\\results\\bird\\text-embedding-3-small-experiments\\5-shot\\text-embedding-3-small-results-5.json 0.453083608459972\n",
      "chapter-3\\results\\bird\\text-embedding-ada-002-experiments\\1-shot\\text-embedding-ada-002-results-1.json 0.4515177439394441\n",
      "chapter-3\\results\\bird\\text-embedding-ada-002-experiments\\3-shot\\text-embedding-ada-002-results-3.json 0.4569500716938551\n",
      "chapter-3\\results\\bird\\text-embedding-ada-002-experiments\\5-shot\\text-embedding-ada-002-results-5.json 0.4552924421499138\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import glob\n",
    "import os\n",
    "from utils.utils import mask_query, sql_similarity\n",
    "\n",
    "def calculate_mean_example_quality(file_path):\n",
    "    # Load the JSON file\n",
    "    with open(file_path, 'r') as file:\n",
    "        data = json.load(file)\n",
    "    \n",
    "    total_quality = 0\n",
    "    total_questions = len(data['questions'])\n",
    "    \n",
    "    for question in data['questions']:\n",
    "        example_qualities = []\n",
    "        for example in question['examples']:\n",
    "            masked_query = 'SELECT' + mask_query(question['response'])\n",
    "            masked_example_query = mask_query(example['query'])\n",
    "            quality = sql_similarity(masked_query, masked_example_query)\n",
    "            example_qualities.append(quality)\n",
    "        \n",
    "        mean_quality = sum(example_qualities) / len(example_qualities)\n",
    "        total_quality += mean_quality\n",
    "    \n",
    "    overall_mean_quality = total_quality / total_questions\n",
    "    return overall_mean_quality\n",
    "\n",
    "\n",
    "# Retrieve all local json results-file paths from completed Spider runs.\n",
    "pattern = os.path.join('chapter-3','results','bird', '**', '*results-[12345].json')\n",
    "\n",
    "results = glob.glob(pattern, recursive=True)\n",
    "\n",
    "# Tally up how many of the 1034 Spider questions have been answered correctly\n",
    "\n",
    "for file_path in results:\n",
    "    print(file_path, calculate_mean_example_quality(file_path))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BIRD % of Questions with a 'Good' Example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0.75 Threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1534/1534 [37:35:56<00:00, 88.24s/it]      "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Proportion of BIRD test questions with a 'good' training example: 0.88\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from utils.utils import sql_similarity, mask_query\n",
    "from tqdm import tqdm\n",
    "\n",
    "def read_sql_queries(file_path, has_db_name=False):\n",
    "    with open(file_path, 'r') as file:\n",
    "        lines = file.readlines()\n",
    "    if has_db_name:\n",
    "        queries = [line.split('\\t')[0].strip() for line in lines]\n",
    "    else:\n",
    "        queries = [line.strip() for line in lines]\n",
    "    return queries\n",
    "\n",
    "def calculate_proportion(dev_file, train_gold_file, threshold=0.75):\n",
    "    dev_queries = read_sql_queries(dev_file, has_db_name=True)\n",
    "    train_gold_queries = read_sql_queries(train_gold_file)\n",
    "    \n",
    "    masked_train = [mask_query(query) for query in train_gold_queries]\n",
    "    embeddings_train = embedding.encode(masked_train)\n",
    "    \n",
    "    matching_count = 0\n",
    "    \n",
    "    for dev_query in tqdm(dev_queries):\n",
    "        \n",
    "        # Firstly order candidates by fine-tuned SQL embedding distances\n",
    "        masked_dev = mask_query(dev_query)\n",
    "        embedding_dev = embedding.encode(masked_dev).reshape(1, -1)\n",
    "        \n",
    "        distances = np.squeeze(euclidean_distances(embedding_dev, embeddings_train)).tolist()\n",
    "        pairs = [(distance, index) for distance, index in zip(distances, range(len(distances)))]\n",
    "\n",
    "        pairs_sorted = sorted(pairs, key=lambda x: x[0])\n",
    "        for d, index in pairs_sorted:\n",
    "            # Determine the SQLSim value and if it is above the threshold, record a success and move on to the next question\n",
    "            if sql_similarity(masked_dev, masked_train[index]) > 0.75:\n",
    "                matching_count += 1\n",
    "                break\n",
    "    \n",
    "    proportion = matching_count / len(dev_queries)\n",
    "    return proportion\n",
    "\n",
    "dev_file_path = 'benchmarks/bird/dev.sql'\n",
    "train_gold_file_path = 'benchmarks/bird/train_gold.sql'\n",
    "\n",
    "proportion = calculate_proportion(dev_file_path, train_gold_file_path)\n",
    "print(f\"Proportion of BIRD test questions with a 'good' training example: {proportion:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0.85 Threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1534/1534 [20:06<00:00,  1.27it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Proportion of BIRD test questions with a 'good' training example: 0.66\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from utils.utils import sql_similarity, mask_query\n",
    "from tqdm import tqdm\n",
    "\n",
    "def read_sql_queries(file_path, has_db_name=False):\n",
    "    with open(file_path, 'r') as file:\n",
    "        lines = file.readlines()\n",
    "    if has_db_name:\n",
    "        queries = [line.split('\\t')[0].strip() for line in lines]\n",
    "    else:\n",
    "        queries = [line.strip() for line in lines]\n",
    "    return queries\n",
    "\n",
    "def calculate_proportion(dev_file, train_gold_file, threshold=0.75):\n",
    "    dev_queries = read_sql_queries(dev_file, has_db_name=True)\n",
    "    train_gold_queries = read_sql_queries(train_gold_file)\n",
    "    \n",
    "    masked_train = [mask_query(query) for query in train_gold_queries]\n",
    "    embeddings_train = embedding.encode(masked_train)\n",
    "    \n",
    "    matching_count = 0\n",
    "    \n",
    "    for dev_query in tqdm(dev_queries):\n",
    "        \n",
    "        # Firstly order candidates by fine-tuned SQL embedding distances\n",
    "        masked_dev = mask_query(dev_query)\n",
    "        embedding_dev = embedding.encode(masked_dev).reshape(1, -1)\n",
    "        \n",
    "        distances = np.squeeze(euclidean_distances(embedding_dev, embeddings_train)).tolist()\n",
    "        pairs = [(distance, index) for distance, index in zip(distances, range(len(distances)))]\n",
    "\n",
    "        pairs_sorted = sorted(pairs, key=lambda x: x[0])\n",
    "        for d, index in pairs_sorted:\n",
    "            # Determine the SQLSim value and if it is above the threshold, record a success and move on to the next question\n",
    "            if sql_similarity(masked_dev, masked_train[index]) > 0.85:\n",
    "                matching_count += 1\n",
    "                break\n",
    "    \n",
    "    proportion = matching_count / len(dev_queries)\n",
    "    return proportion\n",
    "\n",
    "dev_file_path = 'benchmarks/bird/dev.sql'\n",
    "train_gold_file_path = 'benchmarks/bird/train_gold.sql'\n",
    "\n",
    "proportion = calculate_proportion(dev_file_path, train_gold_file_path)\n",
    "print(f\"Proportion of BIRD test questions with a 'good' training example: {proportion:.2f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
